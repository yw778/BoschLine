{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "%matplotlib notebook\n",
    "# numpy, matplotlib, seaborn and matplotlib\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# missing data visualization library\n",
    "import missingno as msno\n",
    "sns.set_style('whitegrid')\n",
    "# define data path\n",
    "INPUT_PATH = \"../data/%s.csv\"\n",
    "# define small samples to quickly explore data\n",
    "read_rows = 300;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Visualization Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview Part\n",
    "This part is to overview the first N rows of the sample to get initial feeling of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data head for overview\n",
    "train_date_overview = pd.read_csv('../data/train_date.csv',nrows = read_rows)\n",
    "#train_categorical_overview = pd.read_csv('../data/train_categorical.csv',nrows = read_rows)\n",
    "#train_numeric_overview = pd.read_csv('../data/train_numeric.csv',nrows = read_rows)\n",
    "#test_categorical_overview = pd.read_csv('../data/test_categorical.csv', nrows = read_rows)\n",
    "#test_date_overview = pd.read_csv('../data/test_date.csv', nrows = read_rows)\n",
    "#test_numeric_overview = pd.read_csv('../data/test_numeric.csv', nrows = read_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train_numeric_overview\n",
    "#train_categorical_overview\n",
    "#train_date_overview\n",
    "#test_numeric_overview\n",
    "#test_categorical_overview\n",
    "#test_date_overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Feature Exploration Part\n",
    "This part is to explore the common feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate failure rate\n",
    "def get_failure_rate(file_name):\n",
    "    # will calculate the error rate from data on the file based on response col\n",
    "    # Respons: 1 = Failed QC , 0 = Passed QC\n",
    "    rows = pd.read_csv(INPUT_PATH % file_name, usecols=[\"Response\"])\n",
    "    failure_rate = rows[rows.Response == 1].size / float(rows[rows.Response == 0].size)\n",
    "    return failure_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to calculate data size in 6 dataset\n",
    "def explore_data_size():\n",
    "    # explore the size (rows, cols) of each file\n",
    "    data_files = ['train_numeric', 'train_date', 'train_categorical', 'test_numeric',\n",
    "                  'test_date', 'test_categorical']\n",
    "    stats = []\n",
    "    for file_name in data_files:\n",
    "        cols = pd.read_csv(INPUT_PATH % file_name, nrows=1)\n",
    "        rows = pd.read_csv(INPUT_PATH % file_name, usecols=[\"Id\"])\n",
    "        stats.append({'File': file_name, 'Rows': rows.shape[0], 'Columns': cols.shape[1]})\n",
    "    # convert the result into a DataFrame so we can do plotting.\n",
    "    df = pd.DataFrame(stats, columns=[\"File\", \"Rows\", \"Columns\"])\n",
    "    failure_rate = get_failure_rate('train_numeric')\n",
    "    df[\"Error\"] = 0\n",
    "    df.loc[df.File == 'train_numeric', 'Error'] = failure_rate\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data size\n",
    "explore_data_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Failure Rate for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this part will use function in the belowing part\n",
    "features = pd.read_csv('../data/train_numeric.csv', nrows=1).drop(['Response', 'Id'], axis=1).columns.values\n",
    "line_features, station_features = get_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_error = []\n",
    "for s in station_features:\n",
    "    cols = ['Id', 'Response']\n",
    "    cols.extend(station_features[s])\n",
    "    df = pd.read_csv('../data/train_numeric.csv', usecols=cols).dropna(subset=station_features[s], how='all')\n",
    "    error_rate = df[df.Response == 1].size / float(df[df.Response == 0].size)\n",
    "    station_error.append([df.shape[1]-2, df.shape[0], error_rate]) \n",
    "    \n",
    "station_data = pd.DataFrame(station_error, \n",
    "                         columns=['Features', 'Samples', 'Error_Rate'], \n",
    "                         index=station_features).sort_index()\n",
    "station_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 20))\n",
    "sns.barplot(x='Error_Rate', y=station_data.index.values, data=station_data, color=\"red\")\n",
    "plt.title('Error Rate between Production Stations')\n",
    "# of all the parts that pass through the station, counts the error rate for each station\n",
    "plt.xlabel('Station Error Rate')\n",
    "plt.show()\n",
    "# station 32 used for R&D ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore failure rate with a combination of stations\n",
    "def explore_failure_rate_combine_station(STATIONS):\n",
    "    train_numeric_part = pd.read_csv('../data/train_numeric.csv', nrows=10000)\n",
    "    numeric_cols = train_numeric_part.drop(['Id','Response'], axis=1).count().reset_index().sort_values(by=0, ascending=False)\n",
    "    numeric_cols['station'] = numeric_cols['index'].apply(lambda s: s.split('_')[1])\n",
    "    numeric_cols = numeric_cols[numeric_cols['station'].isin(STATIONS)]\n",
    "    numeric_cols = numeric_cols.drop_duplicates('station', keep='first')['index'].tolist()\n",
    "    \n",
    "    # Load the first feature column\n",
    "    train_numeric = pd.read_csv('../data/train_numeric.csv', usecols=['Id'] + numeric_cols)\n",
    "    train_numeric.columns = ['Id'] + STATIONS\n",
    "    \n",
    "    # Has value ->1 no value ->0\n",
    "    for station in STATIONS:\n",
    "        train_numeric[station] = 1 * (train_numeric[station] >= 0)\n",
    "    # Group by station and output result\n",
    "    response = pd.read_csv('../data/train_numeric.csv', usecols=['Id', 'Response'])\n",
    "    train = response.merge(train_numeric, how='left', on='Id')\n",
    "    train['cnt'] = 1\n",
    "    failure_rate = train.groupby(STATIONS).sum()[['Response', 'cnt']]\n",
    "    failure_rate['failure_rate'] = failure_rate['Response'] / failure_rate['cnt']\n",
    "    return failure_rate\n",
    "    # failure_rate.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_failure_rate = explore_failure_rate_combine_station(['S43','S44'])\n",
    "combined_failure_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# explore min and max value of each station\n",
    "# data = pd.read_csv('../data/train_numeric.csv', nrows=100)\n",
    "\n",
    "# def make_features(df):\n",
    "#     new_features = pd.DataFrame({})\n",
    "#     for s in station_features.keys():\n",
    "#         station_data = df[station_features[s]]\n",
    "#         col = s+'_max'\n",
    "#         new_features[col] = station_data.max(axis=1).fillna(-1.)\n",
    "#         col = s+'_min'\n",
    "#         new_features[col] = station_data.min(axis=1).fillna(-1.)\n",
    "#     return new_features\n",
    "\n",
    "# data = make_features(data)\n",
    "# data.max().max()\n",
    "# data.min().min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Numerical Feature Exploration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_number_features(station_features):\n",
    "    total_features = 0\n",
    "    for key in station_features.keys():\n",
    "        total_features += len(station_features[key]) \n",
    "    return total_features\n",
    "\n",
    "def get_features(feature_list):\n",
    "    # function to group features by station or line of production the convention is:\n",
    "    # L1_S15_F232 means Line 1, Station 15, Feature 232\n",
    "    line_features = {}\n",
    "    station_features = {}\n",
    "    lines = set([item.split('_')[0] for item in feature_list])\n",
    "    stations = set([item.split('_')[1] for item in feature_list])\n",
    "\n",
    "    for l in lines:\n",
    "        line_features[l] = [item for item in feature_list if '%s_' % l in item]\n",
    "\n",
    "    for s in stations:\n",
    "        station_features[s] = [item for item in feature_list if '%s_' % s in item]\n",
    "    \n",
    "    print 'No. of lines: '+ str(len(lines))\n",
    "    print 'No. of stations: '+ str(len(stations))\n",
    "    print 'No. of features: '+ str(int(get_number_features(station_features)))\n",
    "    \n",
    "\n",
    "    return (line_features, station_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_features(data_set_name,is_response):\n",
    "    # how many units processed in each station\n",
    "    # also percentage of failed/passed QS in each station\n",
    "    if is_response :\n",
    "        features = pd.read_csv(INPUT_PATH % data_set_name, nrows=1).drop([\"Response\", \"Id\"], axis=1).columns.values\n",
    "    else:\n",
    "        features = pd.read_csv(INPUT_PATH % data_set_name, nrows=1).drop([\"Id\"], axis=1).columns.values\n",
    "    \n",
    "    line_features, station_features = get_features(features)\n",
    "    # create a dataframe cols: station, features_count\n",
    "    sdf = pd.DataFrame(list({int(key[1:]): len(station_features[key]) for\n",
    "                             key in station_features.keys()}.items()),\n",
    "                       columns=[\"Station\", \"FeatureCount\"])\n",
    "    ldf = pd.DataFrame(list({int(key[1:]): len(line_features[key]) for\n",
    "                             key in line_features.keys()}.items()),\n",
    "                       columns=[\"Line\", \"FeatureCount\"])\n",
    "    \n",
    "    stations_plot = sdf.plot(x=\"Station\", y=\"FeatureCount\", kind=\"bar\",\n",
    "                             title=\"Fig.1 - Features by Station\",\n",
    "                             figsize=(13,6), fontsize=12)\n",
    "    \n",
    "    line_features_plot = ldf.plot(x=\"Line\",y=\"FeatureCount\", kind=\"bar\",\n",
    "                             title=\"Fig.2 - Features by line\",\n",
    "                             figsize=(13,6), fontsize=12)\n",
    "    \n",
    "    #print 'No. of features:'+ str(int(get_number_features(station_features)))\n",
    "    \n",
    "    return line_features, station_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lf, sf = explore_features(\"train_numeric\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to see how many parts per station, how many success and failure parts per station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_product_by_station(data_set_name,read_rows):\n",
    "    # need to see how many product per station\n",
    "    # features = pd.read_csv(INPUT_PATH % data_set_name, nrows=1).drop([\"Response\", \"Id\"], axis=1).columns.values\n",
    "    # line_features, station_features = get_features(features)\n",
    "    station_features = sf\n",
    "    dwb_result = []\n",
    "    ppbs_result = []\n",
    "    rows = 0\n",
    "    # inside this loop we will try to calculate different data sets, this way we only read the\n",
    "    # files once since it takes a while and produce multiple results\n",
    "    for station in station_features:\n",
    "        station_data = pd.read_csv(\n",
    "            INPUT_PATH % data_set_name,\n",
    "            usecols=station_features[station] + ['Id', 'Response'],nrows = read_rows)\n",
    "        \n",
    "        # need to get how many rows in the sample row\n",
    "        if not rows:\n",
    "            rows = station_data.shape[0]\n",
    "        # need to store how many processed units in each station\n",
    "        # only if all features in that station has value we conside unit processed in this station\n",
    "        ppbs_result.append(\n",
    "            {'Station': int(station[1:]),\n",
    "             'Processed': station_data[station_features[station]].\n",
    "             .all(axis=1).sum()})\n",
    "        \n",
    "        # data without blanks (dwb): take all rows and drop any row that has any blank\n",
    "        # in any station column\n",
    "        dwb = station_data.dropna(how=\"any\")\n",
    "        dwb_result.append([int(station[1:]), dwb.shape[0],\n",
    "                       dwb[dwb[\"Response\"] == 1].shape[0],\n",
    "                       dwb[dwb[\"Response\"] == 0].shape[0]])\n",
    "        \n",
    "    # convert list to a dataframe and prepare for the plot\n",
    "    dwb_df = pd.DataFrame(\n",
    "        dwb_result, columns=[\"Station\", \"Count\", \"Failed\", \"Passed\"]).sort_values(by=[\"Station\"])\n",
    "    # calculate the error rate for each station\n",
    "    dwb_df[\"Error\"] = dwb_df.apply(lambda row: float(row[\"Failed\"]) / (row[\"Count\"] + 1), axis=1)\n",
    "    \n",
    "    # plot the stacked bar tot product/ station\n",
    "    my_plot = dwb_df[[\"Station\", \"Failed\", \"Passed\"]].plot(kind=\"bar\", stacked=True, x=\"Station\",\n",
    "        title=\"Fig:3 - Products by station\", figsize=(13,6), fontsize=12)\n",
    "    my_plot.set_xlabel(\"Stations\")\n",
    "    my_plot.set_ylabel(\"Record Count\")\n",
    "    \n",
    "    # plot error rate per station\n",
    "    my_plot = dwb_df[[\"Station\", \"Error\"]].plot(\n",
    "        kind=\"bar\", x=\"Station\",\n",
    "        title=\"Fig:4 - Error by station\", figsize=(13,6), fontsize=12)\n",
    "    my_plot.set_xlabel(\"Stations\")\n",
    "    my_plot.set_ylabel(\"Error %\")\n",
    "    \n",
    "    # process the ppbs result to see how many products has been process by each station.\n",
    "    ppbs_df = pd.DataFrame(\n",
    "        ppbs_result, columns=['Station', 'Processed']).sort(columns=['Station'])\n",
    "    # calculate the missed product for each station: all count - processed count\n",
    "    ppbs_df[\"Missed\"] = ppbs_df[\"Processed\"].apply(lambda x: rows - x)\n",
    "    # the plot\n",
    "    ppbs_df.plot(x=\"Station\", kind=\"bar\", stacked=True,\n",
    "            title=\"Fig: 5 - Products processed by each station\",\n",
    "            figsize=(13,6), fontsize=12)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "explore_product_by_station(\"train_numeric\",300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Value exploration by station part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# overview again to compare with latter results\n",
    "train_overview = pd.read_csv(INPUT_PATH % 'train_numeric',nrows = read_rows)\n",
    "train_overview.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# overview again with respect to a station\n",
    "def get_station(dataset, station, res = False, show_figure = True):\n",
    "    overview = pd.read_csv(INPUT_PATH % dataset,nrows = read_rows)\n",
    "#     print overview.head()\n",
    "    if res:\n",
    "        columns = overview.columns.drop([\"Response\", \"Id\"])\n",
    "    else:\n",
    "        columns = overview.columns.drop([\"Id\"])\n",
    "        \n",
    "    stations = set([item.split('_')[1] for item in list(columns)])\n",
    "    station_features = {}\n",
    "    for s in stations:\n",
    "        station_features[s] = [item for item in list(columns) if '%s_' % s in item]  \n",
    "#     print overview[station_features[station]].head()\n",
    "    print np.unique(overview[station_features[station]].values)\n",
    "    \n",
    "    overview_explore_nan = pd.DataFrame(columns = stations) \n",
    "    # prepare for dataset with one station\n",
    "    # if contain values not nan\n",
    "    # if no values nan\n",
    "    for s in stations:\n",
    "        overview_explore_nan[s] = overview[station_features[s][0]]\n",
    "    overview_explore_nan =overview_explore_nan[sorted(overview_explore_nan.columns, key=lambda item: int(item[1:]))]\n",
    "    # visualize result in flow path\n",
    "    msno.matrix(overview_explore_nan)\n",
    "    # visualize result in bar counting total number of non-nan value\n",
    "    # msno.bar(overview_explore_nan)\n",
    "    # visualize result in correlation coeficient\n",
    "    msno.heatmap(overview_explore_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_numeric\n",
    "# train_date\n",
    "# train_categorical\n",
    "get_station('train_numeric', 'S0', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get_station('train_date', 'S1', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_station('train_categorical', 'S32', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Categorial Data Exploration Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lf, sf = explore_features(\"train_categorical\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this function is to explore sparsity rate per row\n",
    "def explore_sparse_rate(data_set_name, read_rows):\n",
    "    train_categorical = pd.read_csv(INPUT_PATH % data_set_name,nrows = read_rows).drop([\"Id\"], axis=1)\n",
    "    sparse_rate_each_part = []\n",
    "    sparse_rate_series = train_categorical.isnull().sum(axis = 1)/train_categorical.shape[1]\n",
    "    sparse_rate_df = pd.DataFrame(\n",
    "        sparse_rate_series, columns=['sparse_rate'])\n",
    "    return sparse_rate_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore_sparse_rate('train_categorical', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function is to get feeling of the TX value in the dataset\n",
    "def explore_dintinct_Tx_value(data_set_name, read_rows):\n",
    "    train_categorical = pd.read_csv(INPUT_PATH % 'train_categorical',nrows = read_rows)\n",
    "    del train_categorical['Id']\n",
    "    train_categorical_zeroes = train_categorical.fillna(0)\n",
    "    train_categorical_zeroes = train_categorical_zeroes.values \n",
    "    return np.unique(train_categorical_zeroes[train_categorical_zeroes!=0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explore_dintinct_Tx_value('train_categorical', 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Enginnering Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Method 1 : Applying PCA for Dimension Reduction\n",
    "Preliminary work to search for appropriate n component\n",
    "benchmark: how many variance is explained or we maintain how much variance\n",
    "This model isn't added for feature engineeing by now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import additional pakage\n",
    "import math\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "from sklearn import preprocessing\n",
    "# define chunksize how many rows load one time\n",
    "chunksize  = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the column headers from first rows\n",
    "# Reading as float32 to save Memory \n",
    "data = pd.read_csv(INPUT_PATH % 'train_numeric',nrows=1)\n",
    "float_cols = [c for c in data]\n",
    "float32_cols = {c: np.float32 for c in float_cols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define the number of pc for preliminary decomposition\n",
    "n_components = 968\n",
    "predictors = [x for x in data.keys() if (x != 'Response' and x != 'Id')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard_feature_scaling(df):\n",
    "    std_scale = preprocessing.StandardScaler().fit(df)\n",
    "    df_std = pd.DataFrame(std_scale.transform(df))\n",
    "    df_std.columns = df.columns\n",
    "    df_std.index = df.index\n",
    "    return df_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mean_imputer(DF):\n",
    "    fill_NaN = preprocessing.Imputer(missing_values=np.nan, strategy='median')\n",
    "    imputed_DF = pd.DataFrame(fill_NaN.fit_transform(DF))\n",
    "    imputed_DF.columns = DF.columns\n",
    "    imputed_DF.index = DF.index\n",
    "    return imputed_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ipca = IncrementalPCA(n_components=968)\n",
    "# Due to the large dataset read data seperately in chunks and perform IncrementalPCA \n",
    "counter = 0\n",
    "for chunk in pd.read_csv(INPUT_PATH % 'train_numeric', chunksize=chunksize,dtype=float32_cols):\n",
    "    counter += chunksize\n",
    "    print ('processed',counter,'samples')\n",
    "# To do: what value to fill NA \n",
    "    imputed_chunk = chunk[predictors].fillna(0.09)\n",
    "    #imputed_chunk = mean_imputer(chunk[predictors])\n",
    "    scailed_chunk = standard_feature_scaling(imputed_chunk)\n",
    "#    chunk  = scailed_chunk.fillna(0)\n",
    "    ipca.partial_fit(scailed_chunk)\n",
    "print ('Number of Samples Seen:',ipca.n_samples_seen_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_explained_variance(pca_components):\n",
    "    #print ('Explained variance by %d PCs:' %pca_components, np.sum(ipca.explained_variance_ratio_[:pca_components]))\n",
    "    return np.sum(ipca.explained_variance_ratio_[:pca_components])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot how many variance is explained based on how many n_components\n",
    "n_components_value = range(968)\n",
    "n_components_explained_variance = [get_explained_variance(x) for x in n_components_value]\n",
    "explained_variance_pd = pd.DataFrame(n_components_explained_variance, columns=['explained_variance_rate'])\n",
    "my_plot = explained_variance_pd.plot(title=\"explained_variance_rate\", figsize=(10,6), fontsize=12)\n",
    "my_plot.set_ylabel(\"Explained Variance Rate\")\n",
    "my_plot.set_xlabel(\"Number of Component\")\n",
    "n_components_explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make Train DataFrame only with n PC \n",
    "PC_n = ['f'+str(x) for x in range(0,n_components)]\n",
    "date_final = pd.DataFrame(columns=PC_n)\n",
    "for cat in pd.read_csv(INPUT_PATH % 'train_numeric', chunksize=chunksize,dtype=float32_cols):\n",
    "        cat  = cat.fillna(999)\n",
    "        y=ipca.transform(cat[predictors])\n",
    "        temp = cat['Id'].to_frame()\n",
    "        for i in PC_n:\n",
    "            temp[i]=0\n",
    "        temp[PC_n]=y\n",
    "        date_final = date_final.merge(temp, how='outer')\n",
    "print (date_final)\n",
    "date_final['Id'] = date_final['Id'].astype(np.int32)\n",
    "# save new dataset to CSV file \n",
    "train_filename = 'train_numeric_SVD_%s.csv' % n_components\n",
    "#date_final.to_csv(INPUT_PATH % train_filename,index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Test DataFrame only with n PC \n",
    "PC_n = ['f'+str(x) for x in range(0,n_components)]\n",
    "date_final = pd.DataFrame(columns=PC_n)\n",
    "for cat in pd.read_csv('../input/test_numeric.csv', chunksize=chunksize,dtype=float32_cols):\n",
    "        cat  = cat.fillna(999)\n",
    "        y=ipca.transform(cat[predictors])\n",
    "        temp = cat['Id'].to_frame()\n",
    "        for i in PC_n:\n",
    "            temp[i]=0\n",
    "        temp[PC_n]=y\n",
    "        date_final = date_final.merge(temp, how='outer')\n",
    "date_final['Id'] = date_final['Id'].astype(np.int32)\n",
    "test_filename = 'test_numeric_SVD_%s.csv' % n_components\n",
    "# Save dataset to CSV file\n",
    "#date_final.to_csv(INPUT_PATH % test_filename,index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 : XGBoost to select important features\n",
    "\n",
    "choose 25 top date and numerical features reported by initial xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import additional package\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample the data in a roundabout way to get 5 percent sample\n",
    "date_chunks = pd.read_csv(\"../data/train_date.csv\", index_col=0, chunksize=10000, dtype=np.float32)\n",
    "num_chunks = pd.read_csv(\"../data/train_numeric.csv\", index_col=0,\n",
    "                         usecols=list(range(969)), chunksize=10000, dtype=np.float32)\n",
    "df_train = pd.concat([pd.concat([dchunk, nchunk], axis=1).sample(frac=0.05)\n",
    "               for dchunk, nchunk in zip(date_chunks, num_chunks)])\n",
    "y_sample = pd.read_csv(\"../data/train_numeric.csv\", index_col=0, usecols=[0,969], dtype=np.float32).loc[df_train.index].values.ravel()\n",
    "x_sample = df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # now just numerical feature to see the result\n",
    "# num_chunks = pd.read_csv(\"../data/train_numeric.csv\", index_col=0,\n",
    "#                          usecols=list(range(969)), chunksize=10000, dtype=np.float32)\n",
    "# df_train = pd.concat([nchunk for nchunk in num_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO fit y ravel or not?\n",
    "clf = XGBClassifier(base_score=0.005)\n",
    "clf.fit(x_sample, y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fea_importance = pd.DataFrame(clf.feature_importances_)\n",
    "fea_importance.index = df_train.columns\n",
    "fea_importance.columns=[\"feature_importance\"]\n",
    "# select 25 most important feature for xgboost\n",
    "fea_importance_sorted = fea_importance.sort(['feature_importance'], ascending=[0])\n",
    "important_feature_num_date = fea_importance_sorted.ix[0:25,0]\n",
    "important_feature_num_date.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_feature_value = pd.read_csv(INPUT_PATH % 'train_date',nrows=1).columns.drop(['Id']).values.tolist()\n",
    "important_feature_num_list = list(set(important_feature_num_date.index.values.tolist())-set(date_feature_value))\n",
    "important_feature_date_list = list(set(important_feature_num_date.index.values.tolist())-set(important_feature_num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load entire dataset for these features. \n",
    "X = np.concatenate([\n",
    "    pd.read_csv(\"../data/train_date.csv\", index_col=0, dtype=np.float32,\n",
    "                usecols=['Id']+important_feature_date_list).values,\n",
    "    pd.read_csv(\"../data/train_numeric.csv\", index_col=0, dtype=np.float32,\n",
    "                usecols=['Id']+important_feature_num_list).values\n",
    "], axis=1)\n",
    "\n",
    "y = pd.read_csv(\"../data/train_numeric.csv\", index_col=0, dtype=np.float32, usecols=[0,969]).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create important feature dataframe and plot\n",
    "fea_importance = pd.DataFrame(clf.feature_importances_)\n",
    "fea_importance.index = df_train.columns\n",
    "fea_importance.columns=[\"feature_importance\"]\n",
    "\n",
    "fea_importance_new = fea_importance[fea_importance.feature_importance >0.005]\n",
    "fea_importance_new.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# threshold for a manageable number of features\n",
    "plt.hist(clf.feature_importances_[clf.feature_importances_>0])\n",
    "important_indices = np.where(clf.feature_importances_>0.005)[0]\n",
    "print(important_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load entire dataset for these features. \n",
    "# note where the feature indices are split so we can load the correct ones straight from read_csv\n",
    "n_date_features = 1156\n",
    "X = np.concatenate([\n",
    "    pd.read_csv(\"../data/train_date.csv\", index_col=0, dtype=np.float32,\n",
    "                usecols=np.concatenate([[0], important_indices[important_indices < n_date_features] + 1])).values,\n",
    "    pd.read_csv(\"../data/train_numeric.csv\", index_col=0, dtype=np.float32,\n",
    "                usecols=np.concatenate([[0], important_indices[important_indices >= n_date_features] + 1 - n_date_features])).values\n",
    "], axis=1)\n",
    "\n",
    "y = pd.read_csv(\"../data/train_numeric.csv\", index_col=0, dtype=np.float32, usecols=[0,969]).values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "# select top 20 important numerical feature\n",
    "num_chunks = pd.read_csv(\"../data/train_numeric.csv\", index_col=0,\n",
    "                         usecols=list(range(969)), chunksize=10000, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 : Magic Feature \n",
    "This features are reported by Faron in the forum\n",
    "\n",
    "Start time.   Component(part) production start time\n",
    "\n",
    "MF1.  difference of Id between current and previous rows \n",
    "\n",
    "MF2.  difference of Id between current and next rows \n",
    "\n",
    "MF3.  difference of Id between current and previous rows after sorting on increasing StartTime and Id\n",
    "\n",
    "MF4.  difference of Id between current and next rows after sorting on increasing StartTime and Id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID_COLUMN = 'Id'\n",
    "TARGET_COLUMN = 'Response'\n",
    "train = pd.read_csv(INPUT_PATH %'train_numeric', usecols=[ID_COLUMN, TARGET_COLUMN])\n",
    "\n",
    "test = pd.read_csv(INPUT_PATH %'test_numeric', usecols=[ID_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Time feature\n",
    "train[\"StartTime\"] = -1\n",
    "test[\"StartTime\"] = -1\n",
    "# Duration time feature\n",
    "train[\"Duration\"] = -1\n",
    "test[\"Duration\"] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in start time\n",
    "for tr, te in zip(pd.read_csv(INPUT_PATH %'train_date', chunksize=50000), pd.read_csv(INPUT_PATH %'test_date', chunksize=50000)):\n",
    "    #pick all columns except ID\n",
    "    feats = np.setdiff1d(tr.columns, [ID_COLUMN])\n",
    "\n",
    "    stime_tr = tr[feats].min(axis=1).values\n",
    "    stime_te = te[feats].min(axis=1).values\n",
    "    \n",
    "    etime_tr = tr[feats].max(axis=1).values\n",
    "    etime_te = te[feats].max(axis=1).values\n",
    "\n",
    "    train.loc[train.Id.isin(tr.Id), 'StartTime'] = stime_tr\n",
    "    test.loc[test.Id.isin(te.Id), 'StartTime'] = stime_te\n",
    "    \n",
    "    train.loc[train.Id.isin(tr.Id), 'Duration'] = etime_tr - stime_tr\n",
    "    test.loc[test.Id.isin(te.Id), 'Duration'] = etime_te - stime_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "train_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n",
    "# Begin to create MF1 & MF2\n",
    "train_test['MF1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
    "train_test['MF2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
    "\n",
    "train_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n",
    "# Begin to create MF3 & MF4\n",
    "train_test['MF3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
    "train_test['MF4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
    "\n",
    "train_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\n",
    "train = train_test.iloc[:ntrain, :]\n",
    "test = train_test.iloc[ntrain:, :]\n",
    "\n",
    "features = np.setdiff1d(list(train.columns), [TARGET_COLUMN, ID_COLUMN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_magic_feature_train = train[features]\n",
    "df_magic_feature_test = test[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_magic_feature_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ntrain = train.shape[0]\n",
    "# train_test = pd.concat((train, test)).reset_index(drop=True).reset_index(drop=False)\n",
    "# # Begin to create MF1 & MF2\n",
    "# train_test['MF1'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
    "# train_test['MF2'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
    "\n",
    "# train_test = train_test.sort_values(by=['StartTime', 'Id'], ascending=True)\n",
    "# # Begin to create MF3 & MF4\n",
    "# train_test['MF3'] = train_test[ID_COLUMN].diff().fillna(9999999).astype(int)\n",
    "# train_test['MF4'] = train_test[ID_COLUMN].iloc[::-1].diff().fillna(9999999).astype(int)\n",
    "\n",
    "# train_test = train_test.sort_values(by=['index']).drop(['index'], axis=1)\n",
    "# train = train_test.iloc[:ntrain, :]\n",
    "\n",
    "# features = np.setdiff1d(list(train.columns), [TARGET_COLUMN, ID_COLUMN])\n",
    "\n",
    "# y = train.Response.ravel()\n",
    "# train = np.array(train[features])\n",
    "\n",
    "# print('train: {0}'.format(train.shape))\n",
    "# prior = np.sum(y) / (1.*len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all features and Finish feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_feature will be the ensembled feature afterwards\n",
    "train_feature = pd.DataFrame(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put important feature in the dataframe \n",
    "train_feature.columns = important_feature_num_date.index\n",
    "train_feature = pd.concat([train_feature,df_magic_feature_train],axis=1)\n",
    "#end of feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Engineered features from R and feature tuning\n",
    "\n",
    "These are currently used feature from the R script\n",
    "see feature engineering R file in the same directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl_df = pd.read_csv(\"../input/engineered_feature.csv\", index_col=0, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choosing with different feature subset\n",
    "tl_df.columns\n",
    "# tl_df = tl_df[['target', 'P1', 'ord', 'group_len', 'fst', 'S.S32.mean',\n",
    "#        'S.S33.mean', 'S.S38.mean', 'L3.fst', 'L3.lst', 'time_dtL3',\n",
    "#        'time_idtL3', 'NAs_dtL3', 'NAs_idtL3','target_prev', 'target_next']]\n",
    "# tl_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tl_train = tl_df[tl_df[\"target\"].notnull()]\n",
    "target_y = tl_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del tl_train[\"target\"]\n",
    "tl_test = tl_df[tl_df[\"target\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tl_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# self-defined xgb model wrapper including data processing, model fit, predict and result \n",
    "def modelfit(alg, dtrain, labels,useTrainCV=True, model_fit=True, fea_impor=True, model_predict=True,cv_folds=5, early_stopping_rounds=30):\n",
    "    # trainCV with early stop so we don't need to use sciki-learn GridsearchCV , the later can't run in my computer\n",
    "    best_threshold = 0.5\n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(dtrain.values, label=labels)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "#         print cvresult\n",
    "#         return\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        plt.figure(8)\n",
    "        plt.plot(range(alg.get_params()['n_estimators']), cvresult[\"train-auc-mean\"].values)\n",
    "        plt.plot(range(alg.get_params()['n_estimators']), cvresult[\"test-auc-mean\"].values)\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.xlabel('training rounds')\n",
    "        plt.ylabel('Auc')\n",
    "        plt.show()\n",
    "    if model_fit:\n",
    "        #Fit the algorithm on the data\n",
    "        alg.fit(dtrain.values, labels, eval_metric='auc')\n",
    "        print (\"fit done\")\n",
    "    if fea_impor:\n",
    "        plt.figure(9)\n",
    "        feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "        feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "        plt.ylabel('Feature Importance Score')\n",
    "        plt.show()\n",
    "    if model_predict:\n",
    "    #Predict training set:\n",
    "        dtrain_predprob = alg.predict_proba(dtrain.values)[:,1]\n",
    "        thresholds = np.linspace(0.01, 0.99, 50)\n",
    "        mcc = np.array([matthews_corrcoef(labels, dtrain_predprob>thr) for thr in thresholds])    \n",
    "        plt.figure(10)\n",
    "        plt.plot(thresholds, mcc)\n",
    "        plt.show()\n",
    "        best_threshold = thresholds[mcc.argmax()]\n",
    "        print('MCC is' + str(mcc.max()))\n",
    "        print('best threshold is'+ str(best_threshold))\n",
    "    return best_threshold\n",
    "    \n",
    "        \n",
    "    #Print model report:\n",
    "#     print (\"\\nModel Report\")\n",
    "#     print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[target].values, dtrain_predictions))\n",
    "#     print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain[target], dtrain_predprob))\n",
    "                    \n",
    "#     feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Training and Cross Validation Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_starter = XGBClassifier(\n",
    " learning_rate =0.03,\n",
    " n_estimators=105,\n",
    " max_depth=9,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.6,\n",
    " objective= 'reg:linear',\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=0,\n",
    " base_score=0.005)\n",
    "\n",
    "\n",
    "train_x = tl_train.values\n",
    "train_y = target_y.values\n",
    "report_out_of_fold_score(xgb_starter)\n",
    "# best_threshold = modelfit(xgb_starter, tl_train, target_y.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_out_of_fold_score(clf_test):\n",
    "    n_folds=2\n",
    "    skf = StratifiedKFold(target_y, n_folds)\n",
    "\n",
    "\n",
    "    preds = np.ones(train_y.shape[0])\n",
    "    aucs=np.ones(n_folds)\n",
    "    mccs=np.ones(n_folds)\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Fold\", i\n",
    "        clf_test.fit(train_x[train], train_y[train])\n",
    "        print(\"fold {}, best round: {}\".format(i, clf_test.best_round))\n",
    "        preds[test] =clf_test.predict_proba(train_x[test])[:,1]\n",
    "        print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(train_y[test], preds[test])))\n",
    "        thresholds = np.linspace(0.01, 0.99, 50)\n",
    "        mcc = np.array([matthews_corrcoef(train_y[test], preds[test]>thr) for thr in thresholds])  \n",
    "        print(\"fold {}, MCC: MAX {:.3f}\".format(i,mcc.max()))\n",
    "        aucs[i]=roc_auc_score(train_y[test], preds[test])\n",
    "        mccs[i]=mcc.max()\n",
    "    print(\"mean out of fold AUC \"+str(np.mean(aucs)))\n",
    "    print(\"mean out of fold MCC \"+str(np.mean(mccs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def report_out_of_fold_score_predict(clf_test):\n",
    "    n_folds=5\n",
    "    skf = StratifiedKFold(target_y, n_folds)\n",
    "\n",
    "\n",
    "    preds = np.ones(train_y.shape[0])\n",
    "    aucs=np.ones(n_folds)\n",
    "    mccs=np.ones(n_folds)\n",
    "    for i, (train, test) in enumerate(skf):\n",
    "        print \"Fold\", i\n",
    "        clf_test.fit(train_x[train], train_y[train])\n",
    "        preds[test] = clf_test.predict(train_x[test])\n",
    "        print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(train_y[test], preds[test])))\n",
    "#         thresholds = np.linspace(0.01, 0.99, 50)\n",
    "        mcc = matthews_corrcoef(train_y[test], preds[test])\n",
    "        print(\"fold {}, MCC: MAX {:.3f}\".format(i,mcc))\n",
    "        aucs[i]=roc_auc_score(train_y[test], preds[test])\n",
    "        mccs[i]=mcc\n",
    "    print(\"mean out of fold AUC \"+str(np.mean(aucs)))\n",
    "    print(\"mean out of fold MCC \"+str(np.mean(mccs)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 random forest\n",
    "This model gived AUC  (91%), MCC (MCC 0.47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef, roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strategy_imputer(DF,strategy=\"median\"):\n",
    "    fill_NaN = preprocessing.Imputer(missing_values=np.nan, strategy=strategy)\n",
    "    imputed_DF = pd.DataFrame(fill_NaN.fit_transform(DF))\n",
    "    imputed_DF.columns = DF.columns\n",
    "    imputed_DF.index = DF.index\n",
    "    return imputed_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tl_df = pd.read_csv(\"../input/engineered_feature.csv\", index_col=0, dtype=np.float32)\n",
    "train_index = tl_df[\"target\"].notnull()\n",
    "test_index = tl_df[\"target\"].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# fill in missing values before RF\n",
    "# tl_imputed=strategy_imputer(tl_df)\n",
    "tl_imputed = tl_df.fillna(0)\n",
    "tl_train = tl_imputed[train_index]\n",
    "tl_test = tl_imputed[test_index]\n",
    "target_y = tl_train[\"target\"]\n",
    "del tl_train[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# convert into ndarray\n",
    "train_x = tl_train.values\n",
    "train_y = target_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_folds=5\n",
    "skf = StratifiedKFold(target_y, n_folds)\n",
    "# clf = RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='entropy')\n",
    "clf_fold = RandomForestClassifier(n_estimators = 100, n_jobs = -1,random_state =1, \n",
    "                             max_features = \"auto\", min_samples_leaf = ,criterion='entropy' )\n",
    "preds = np.ones(train_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_rf_entropy = RandomForestClassifier(n_estimators = 100, n_jobs = -1,random_state =1, \n",
    "                             max_features = \"auto\", min_samples_leaf = 3,\n",
    "                             criterion='entropy')\n",
    "report_out_of_fold_score(clf_rf_entropy)\n",
    "# clf_rf_entropy.fit(train_x,train_y)\n",
    "# pred = clf_rf_entropy.predict_proba(train_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_rf_gini = RandomForestClassifier(n_estimators = 100, n_jobs = -1,random_state =1, \n",
    "                             max_features = \"auto\", min_samples_leaf = 3,criterion='gini')\n",
    "report_out_of_fold_score(clf_rf_gini)\n",
    "# clf_rf_gini.fit(train_x,train_y)\n",
    "# pred = clf_rf_gini.predict_proba(train_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build a classifier and verify by build in oob_score\n",
    "clfa = RandomForestClassifier(n_estimators = 100, oob_score = True, n_jobs = -1,random_state =1, \n",
    "                             max_features = \"auto\", min_samples_leaf = 3,criterion='entropy')\n",
    "clfa.fit(train_x,train_y)\n",
    "print \"AUC - ROC : \", clfa.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = clfa.predict_proba(train_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# verify by out-of-fold cv and MCC\n",
    "# new findng: out-of-fold MCC as determined way\n",
    "aucs=np.ones(n_folds)\n",
    "mccs=np.ones(n_folds)\n",
    "for i, (train, test) in enumerate(skf):\n",
    "    print \"Fold\", i\n",
    "#     X_train = train_x[train]\n",
    "#     y_train = train_y[train]\n",
    "#     X_test = train_x[test]\n",
    "#     y_test = train_y[test]\n",
    "    preds[test] = clf_fold.fit(train_x[train], train_y[train]).predict_proba(train_x[test])[:,1]\n",
    "#     preds[test] = clf.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "    print(\"fold {}, ROC AUC: {:.3f}\".format(i, roc_auc_score(train_y[test], preds[test])))\n",
    "    thresholds = np.linspace(0.01, 0.99, 50)\n",
    "    mcc = np.array([matthews_corrcoef(train_y[test], preds[test]>thr) for thr in thresholds])  \n",
    "    print(\"fold {}, MCC: MAX {:.3f}\".format(i,mcc.max()))\n",
    "    aucs[i]=roc_auc_score(train_y[test], preds[test])\n",
    "    mccs[i]=mcc.max()\n",
    "#     print(roc_auc_score(y, preds))\n",
    "#     y_submission = clf.predict_proba(X_test)[:, 1]\n",
    "#     #         print clf.predict_proba(X_test)[:, 1]\n",
    "#     #         break\n",
    "#     dataset_blend_train[test, j] = y_submission\n",
    "#     dataset_blend_test_j[:, i] = clf.predict_proba(X_submission)[:, 1]\n",
    "#         print y_submission\n",
    "# thresholds = np.linspace(0.01, 0.99, 50)\n",
    "# mcc = np.array([matthews_corrcoef(train_y, preds>thr) for thr in thresholds])\n",
    "# print(\"out of fold auc\"+str(roc_auc_score(train_y, preds))\n",
    "# print(\"out of fold MCC\"+str(mcc.max()))\n",
    "print(\"mean out of fold AUC \"+str(np.mean(aucs)))\n",
    "print(\"mean out of fold MCC \"+str(np.mean(mccs)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.01, 0.99, 50)\n",
    "mcc = np.array([matthews_corrcoef(train_y, pred>thr) for thr in thresholds])    \n",
    "plt.figure(10)\n",
    "plt.plot(thresholds, mcc)\n",
    "plt.show()\n",
    "best_threshold = thresholds[mcc.argmax()]\n",
    "print('MCC is' + str(mcc.max()))\n",
    "print('best threshold is'+ str(best_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 Extra tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "fold 0, ROC AUC: 0.853\n",
      "fold 0, MCC: MAX 0.465\n",
      "Fold 1\n",
      "fold 1, ROC AUC: 0.858\n",
      "fold 1, MCC: MAX 0.475\n",
      "Fold 2\n",
      "fold 2, ROC AUC: 0.869\n",
      "fold 2, MCC: MAX 0.474\n",
      "Fold 3\n",
      "fold 3, ROC AUC: 0.841\n",
      "fold 3, MCC: MAX 0.443\n",
      "Fold 4\n",
      "fold 4, ROC AUC: 0.847\n",
      "fold 4, MCC: MAX 0.471\n",
      "mean out of fold AUC 0.853854562026\n",
      "mean out of fold MCC 0.465634811728\n"
     ]
    }
   ],
   "source": [
    "clf_extr_entropy = ExtraTreesClassifier(n_estimators=30, n_jobs=-1, criterion='entropy',\n",
    "                               max_features=None, min_samples_leaf=3, random_state=0)\n",
    "\n",
    "report_out_of_fold_score(clf_extr_entropy)\n",
    "# clf_extr_entropy.fit(train_x,train_y)\n",
    "# pred = clf_extr_entropy.predict_proba(train_x)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_extr_gini = ExtraTreesClassifier(n_estimators=100, n_jobs=-1, criterion='gini',\n",
    "                               max_features=None, min_samples_split=1, random_state=0)\n",
    "report_out_of_fold_score(clf_extr_gini)\n",
    "# clf_extr_gini.fit(train_x,train_y)\n",
    "# pred = clf_extr_gini.predict_proba(train_x)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4 LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pylightgbm.models import GBMClassifier\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exec_path = \"~/Desktop/project/LightGBM/lightgbm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyLightGBM is looking for 'LIGHTGBM_EXEC' environment variable, cannot be found.\n",
      "exec_path will be deprecated in favor of environment variable\n"
     ]
    }
   ],
   "source": [
    "clf_lightgbm = GBMClassifier(exec_path=exec_path,early_stopping_round=10,verbose=True,\n",
    "                            learning_rate=0.1,num_iterations=1000, min_data_in_leaf=3,metric=\"roc_auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "report_out_of_fold_score(clf_lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_lightgbm.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6 Adaboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyper-Parameter Tuning Part\n",
    "This place is using gridsearch cv for hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyLightGBM is looking for 'LIGHTGBM_EXEC' environment variable, cannot be found.\n",
      "exec_path will be deprecated in favor of environment variable\n",
      "Fitting 2 folds for each of 1 candidates, totalling 2 fits\n",
      "pyLightGBM is looking for 'LIGHTGBM_EXEC' environment variable, cannot be found.\n",
      "exec_path will be deprecated in favor of environment variable\n",
      "pyLightGBM is looking for 'LIGHTGBM_EXEC' environment variable, cannot be found.\n",
      "exec_path will be deprecated in favor of environment variable\n",
      "pyLightGBM is looking for 'LIGHTGBM_EXEC' environment variable, cannot be found.\n",
      "exec_path will be deprecated in favor of environment variable\n"
     ]
    },
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/var/folders/_n/8c5rjjsj4kl5v76kz7z1p9qm0000gn/T/tmp2K1nTU/LightGBM_model.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-ba20dc6f04c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# MCC score for best prediction dermnation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mgrid_search_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-68-ded47a8c2e80>\u001b[0m in \u001b[0;36mgrid_search_report\u001b[0;34m(clf, param_grid, nfold)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgrid_search_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnfold\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Grid score: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[1;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[0;32m--> 553\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m                 for train, test in cv)\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    656\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, error_score)\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python2.7/site-packages/pylightgbm/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, test_data, init_scores)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'output_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/var/folders/_n/8c5rjjsj4kl5v76kz7z1p9qm0000gn/T/tmp2K1nTU/LightGBM_model.txt'"
     ]
    }
   ],
   "source": [
    "#this part is for tune LightGBM using gridSearch CV\n",
    "import numpy as np\n",
    "from sklearn import datasets, metrics, grid_search\n",
    "from pylightgbm.models import GBMClassifier\n",
    "\n",
    "# full path to lightgbm executable (on Windows include .exe)\n",
    "exec_path = \"~/Desktop/project/LightGBM/lightgbm\"\n",
    "\n",
    "gbm = GBMClassifier(exec_path=exec_path,early_stopping_round=10,verbose=True,application='binary')\n",
    "\n",
    "param_grid = {'learning_rate': [0.1], 'num_iterations': np.linspace(85, 95, 1),\n",
    "              'min_data_in_leaf':[3]}\n",
    "\n",
    "scorer = metrics.make_scorer(metrics.matthews_corrcoef, greater_is_better=True)\n",
    "# AUC score for robust determination\n",
    "# clf = grid_search.GridSearchCV(gbm, param_grid, scoring='roc_auc', cv=2,verbose=True)\n",
    "# MCC score for best prediction dermnation\n",
    "clf = grid_search.GridSearchCV(gbm, param_grid, scoring=scorer, cv=2,verbose=True)\n",
    "grid_search_report(clf,param_grid) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "# this function try to find the best paramter combination based \n",
    "# on n-fold cross validation score\n",
    "\n",
    "def grid_search_report(clf, param_grid, nfold=2):\n",
    "    grid_search.GridSearchCV(clf, param_grid, scoring='roc_auc', cv=nfold,verbose=True)\n",
    "    clf.fit(train_x, train_y)\n",
    "    \n",
    "    print(\"Grid score: \",clf.grid_scores_)\n",
    "    print(\"Best score: \", clf.best_score_)\n",
    "    print(\"Best params: \", clf.best_params_)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([mean: 0.89488, std: 0.00288, params: {'learning_rate': 0.1, 'bagging_fraction': 0.5}],\n",
       " {'bagging_fraction': 0.5, 'learning_rate': 0.1},\n",
       " 0.89487916450391936)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.grid_scores_, clf.best_params_, clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load test data, predict labels and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_feature_date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load test data\n",
    "test_feature = np.concatenate([\n",
    "    pd.read_csv(\"../data/test_date.csv\", index_col=0, dtype=np.float32,\n",
    "                usecols=['Id']+important_feature_date_list).values,\n",
    "    pd.read_csv(\"../data/test_numeric.csv\", index_col=0, dtype=np.float32,\n",
    "                usecols=['Id']+important_feature_num_list).values,\n",
    "], axis=1)\n",
    "test_feature = np.concatenate([test_feature,df_magic_feature_test.values],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tl_test = tl_df[tl_df[\"target\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feature = tl_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del test_feature['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feature.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feature.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_feature = mean_imputer(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate predictions at the chosen threshold\n",
    "# preds = (xgb_starter.predict_proba(test_feature)[:,1] > best_threshold).astype(np.int8)\n",
    "preds = (clfa.predict_proba(test_feature)[:,1] > best_threshold).astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and submit\n",
    "sub = pd.read_csv(\"../data/sample_submission.csv\", index_col=0)\n",
    "sub[\"Response\"] = preds\n",
    "sub.to_csv(\"submission.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
